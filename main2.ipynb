{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import re\n",
    "import glob\n",
    "\n",
    "STORE_DECODED_IMAGES = False\n",
    "def preprocess_image(image):\n",
    "    if STORE_DECODED_IMAGES:\n",
    "        return tf.io.parse_tensor(image, tf.uint8)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [331, 331])\n",
    "    return tf.cast(image, tf.uint8)\n",
    "\n",
    "feature_description = {\n",
    "    \"image\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"classname\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"classidx\": tf.io.FixedLenFeature((), tf.int64)}\n",
    "\n",
    "def load_image(example_proto):\n",
    "    ex = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return (preprocess_image(ex[\"image\"]), ex[\"classidx\"])\n",
    "\n",
    "DATADIR = r\"C:\\Users\\91909\\Desktop\\DATA SCIENCE\\DATA\\Petals_to_Metal\\tfrecords-jpeg-331x331\"\n",
    "\n",
    "train_filenames = glob.glob(DATADIR + \"/train/*.tfrec\")\n",
    "test_filenames = glob.glob(DATADIR + \"/test/*.tfrec\")\n",
    "val_filenames = glob.glob(DATADIR + \"/val/*.tfrec\")\n",
    "\n",
    "# Load train dataset\n",
    "train_dataset = tf.data.TFRecordDataset(train_filenames)\n",
    "train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(128).batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = tf.data.TFRecordDataset(test_filenames)\n",
    "test_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32, drop_remainder=False)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = tf.data.TFRecordDataset(val_filenames)\n",
    "val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# Count the total number of images in the dataset\n",
    "def count_dataset_images(filenames):\n",
    "    count = sum(1 for _ in filenames)\n",
    "    return count\n",
    "\n",
    "# Count the number of training and test images\n",
    "train_images = count_dataset_images(train_filenames)\n",
    "test_images = count_dataset_images(test_filenames)\n",
    "\n",
    "print(f'Total training images: {train_images}')\n",
    "print(f'Total test images: {test_images}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: classidx (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 55\u001b[0m\n\u001b[0;32m     52\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m load_dataset(val_filenames)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Count the number of images\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m train_images \u001b[38;5;241m=\u001b[39m \u001b[43mcount_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m test_images \u001b[38;5;241m=\u001b[39m count_images(test_dataset)\n\u001b[0;32m     57\u001b[0m val_images \u001b[38;5;241m=\u001b[39m count_images(val_dataset)\n",
      "Cell \u001b[1;32mIn[71], line 37\u001b[0m, in \u001b[0;36mcount_images\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 37\u001b[0m         batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m         count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Count images in each batch\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3016\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3018\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: classidx (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "# Function to preprocess images\n",
    "def preprocess_image(image):\n",
    "    if STORE_DECODED_IMAGES:\n",
    "        return tf.io.parse_tensor(image, tf.uint8)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [331, 331])\n",
    "    return tf.cast(image, tf.uint8)\n",
    "\n",
    "# Function to load image from TFRecord\n",
    "def load_image(example_proto):\n",
    "    feature_description = {\n",
    "        \"image\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"classname\": tf.io.FixedLenFeature((), tf.string),\n",
    "        \"classidx\": tf.io.FixedLenFeature((), tf.int64)\n",
    "    }\n",
    "    ex = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return (preprocess_image(ex[\"image\"]), ex[\"classidx\"])\n",
    "\n",
    "# Load dataset function\n",
    "def load_dataset(filenames):\n",
    "    dataset = tf.data.TFRecordDataset(filenames)\n",
    "    dataset = dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(32, drop_remainder=False)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return dataset\n",
    "\n",
    "# Function to count images in a dataset\n",
    "def count_images(dataset):\n",
    "    count = 0\n",
    "    iterator = iter(dataset)  # Create an iterator for the dataset\n",
    "    try:\n",
    "        while True:\n",
    "            batch = next(iterator)\n",
    "            count += batch[0].shape[0]  # Count images in each batch\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    return count\n",
    "\n",
    "# Directories\n",
    "DATADIR = r\"C:\\Users\\91909\\Desktop\\DATA SCIENCE\\DATA\\Petals_to_Metal\\tfrecords-jpeg-331x331\"\n",
    "train_filenames = glob.glob(DATADIR + \"/train/*.tfrec\")\n",
    "test_filenames = glob.glob(DATADIR + \"/test/*.tfrec\")\n",
    "val_filenames = glob.glob(DATADIR + \"/val/*.tfrec\")\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = load_dataset(train_filenames)\n",
    "test_dataset = load_dataset(test_filenames)\n",
    "val_dataset = load_dataset(val_filenames)\n",
    "\n",
    "# Count the number of images\n",
    "train_images = count_images(train_dataset)\n",
    "test_images = count_images(test_dataset)\n",
    "val_images = count_images(val_dataset)\n",
    "\n",
    "# Print the results\n",
    "print(f'Total training images: {train_images}')\n",
    "print(f'Total test images: {test_images}')\n",
    "print(f'Total validation images: {val_images}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "STORE_DECODED_IMAGES = False\n",
    "def preprocess_image(image):\n",
    "    if STORE_DECODED_IMAGES:\n",
    "        return tf.io.parse_tensor(image, tf.uint8)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [331, 331])\n",
    "    return tf.cast(image, tf.uint8)\n",
    "\n",
    "feature_description = {\n",
    "    \"image\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"filename\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"classname\": tf.io.FixedLenFeature((), tf.string),\n",
    "    \"classidx\": tf.io.FixedLenFeature((), tf.int64)}\n",
    "\n",
    "def load_image(example_proto):\n",
    "    ex = tf.io.parse_single_example(example_proto, feature_description)\n",
    "    return (preprocess_image(ex[\"image\"]), ex[\"classidx\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = r\"C:\\Users\\91909\\Desktop\\DATA SCIENCE\\DATA\\Petals_to_Metal\\tfrecords-jpeg-331x331\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filenames = glob.glob(DATADIR + \"/train/*.tfrec\")\n",
    "test_filenames = glob.glob(DATADIR + \"/test/*.tfrec\")\n",
    "val_filenames = glob.glob(DATADIR + \"/val/*.tfrec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load train dataset\n",
    "train_dataset = tf.data.TFRecordDataset(train_filenames)\n",
    "train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(128).batch(32, drop_remainder=False)\n",
    "train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Load test dataset\n",
    "test_dataset = tf.data.TFRecordDataset(test_filenames)\n",
    "test_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_dataset = test_dataset.batch(32, drop_remainder=False)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Load validation dataset\n",
    "val_dataset = tf.data.TFRecordDataset(val_filenames)\n",
    "val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(32, drop_remainder=False)\n",
    "val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_SHARDS=10\n",
    "# train_filenames = [DATADIR+\"/train/*.tfrec\".format(i)\n",
    "#                    for i in range(N_SHARDS)]\n",
    "# random.shuffle(train_filenames)\n",
    "\n",
    "# test_filenames = [DATADIR+\"/test/*.tfrec\".format(i)\n",
    "#                    for i in range(N_SHARDS)]\n",
    "# random.shuffle(train_filenames)\n",
    "\n",
    "# val_filenames = [DATADIR+\"/val/*.tfrec\".format(i)\n",
    "#                    for i in range(N_SHARDS)]\n",
    "# random.shuffle(train_filenames)\n",
    "\n",
    "# train_dataset = tf.data.TFRecordDataset(train_filenames)\n",
    "# train_dataset = train_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# train_dataset = train_dataset.shuffle(128).batch(32, drop_remainder=False)\n",
    "# train_dataset = train_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = tf.data.TFRecordDataset(test_filenames)\n",
    "# test_dataset = test_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# test_dataset = test_dataset.shuffle(128).batch(32, drop_remainder=False)\n",
    "# test_dataset = test_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset = tf.data.TFRecordDataset(val_filenames)\n",
    "# val_dataset = val_dataset.map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "# val_dataset = val_dataset.shuffle(128).batch(32, drop_remainder=False)\n",
    "# val_dataset = val_dataset.prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training images: 16\n",
      "Total test images: 16\n"
     ]
    }
   ],
   "source": [
    "# Count the total number of images in the dataset\n",
    "def count_dataset_images(filenames):\n",
    "    count = sum(1 for _ in filenames)\n",
    "    return count\n",
    "\n",
    "# Count the number of training and test images\n",
    "train_images = count_dataset_images(train_filenames)\n",
    "test_images = count_dataset_images(test_filenames)\n",
    "\n",
    "print(f'Total training images: {train_images}')\n",
    "print(f'Total test images: {test_images}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: classidx (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[67], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Fetch a batch of images and labels\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_batch, label_batch \u001b[38;5;129;01min\u001b[39;00m train_dataset\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Convert to numpy arrays\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     image_batch \u001b[38;5;241m=\u001b[39m image_batch\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     14\u001b[0m     label_batch \u001b[38;5;241m=\u001b[39m label_batch\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:766\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    765\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    767\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py:749\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    746\u001b[0m \u001b[38;5;66;03m# TODO(b/77291417): This runs in sync mode as iterators use an error status\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;66;03m# to communicate that there is no more data to iterate over.\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[1;32m--> 749\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    754\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n\u001b[0;32m    756\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_element_spec\u001b[38;5;241m.\u001b[39m_from_compatible_tensor_list(ret)  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py:3016\u001b[0m, in \u001b[0;36miterator_get_next\u001b[1;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[0;32m   3014\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m   3015\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 3016\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3017\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n\u001b[0;32m   3018\u001b[0m   \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\91909\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:7209\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[1;34m(e, name)\u001b[0m\n\u001b[0;32m   7207\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[0;32m   7208\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 7209\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_2_device_/job:localhost/replica:0/task:0/device:CPU:0}} Feature: classidx (data type: int64) is required but could not be found.\n\t [[{{node ParseSingleExample/ParseExample/ParseExampleV2}}]] [Op:IteratorGetNext]"
     ]
    }
   ],
   "source": [
    "def plot_images(images, labels, CLASSES):\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for i in range(len(images)):\n",
    "        plt.subplot(4, 6, i + 1)  # Adjust grid size as needed\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(CLASSES[labels[i]])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Fetch a batch of images and labels\n",
    "for image_batch, label_batch in train_dataset.take(1):\n",
    "    # Convert to numpy arrays\n",
    "    image_batch = image_batch.numpy()\n",
    "    label_batch = label_batch.numpy()\n",
    "\n",
    "    # Define class names (adjust as needed)\n",
    "    CLASSES = ['pink primrose',    'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea',     'wild geranium',     'tiger lily',           'moon orchid',              'bird of paradise', 'monkshood',        'globe thistle',         # 00 - 09\n",
    "           'snapdragon',       \"colt's foot\",               'king protea',      'spear thistle', 'yellow iris',       'globe-flower',         'purple coneflower',        'peruvian lily',    'balloon flower',   'giant white arum lily', # 10 - 19\n",
    "           'fire lily',        'pincushion flower',         'fritillary',       'red ginger',    'grape hyacinth',    'corn poppy',           'prince of wales feathers', 'stemless gentian', 'artichoke',        'sweet william',         # 20 - 29\n",
    "           'carnation',        'garden phlox',              'love in the mist', 'cosmos',        'alpine sea holly',  'ruby-lipped cattleya', 'cape flower',              'great masterwort', 'siam tulip',       'lenten rose',           # 30 - 39\n",
    "           'barberton daisy',  'daffodil',                  'sword lily',       'poinsettia',    'bolero deep blue',  'wallflower',           'marigold',                 'buttercup',        'daisy',            'common dandelion',      # 40 - 49\n",
    "           'petunia',          'wild pansy',                'primula',          'sunflower',     'lilac hibiscus',    'bishop of llandaff',   'gaura',                    'geranium',         'orange dahlia',    'pink-yellow dahlia',    # 50 - 59\n",
    "           'cautleya spicata', 'japanese anemone',          'black-eyed susan', 'silverbush',    'californian poppy', 'osteospermum',         'spring crocus',            'iris',             'windflower',       'tree poppy',            # 60 - 69\n",
    "           'gazania',          'azalea',                    'water lily',       'rose',          'thorn apple',       'morning glory',        'passion flower',           'lotus',            'toad lily',        'anthurium',             # 70 - 79\n",
    "           'frangipani',       'clematis',                  'hibiscus',         'columbine',     'desert-rose',       'tree mallow',          'magnolia',                 'cyclamen ',        'watercress',       'canna lily',            # 80 - 89\n",
    "           'hippeastrum ',     'bee balm',                  'pink quill',       'foxglove',      'bougainvillea',     'camellia',             'mallow',                   'mexican petunia',  'bromelia',         'blanket flower',        # 90 - 99\n",
    "           'trumpet creeper',  'blackberry lily',           'common tulip',     'wild rose']\n",
    "\n",
    "    # Plot images\n",
    "    plot_images(image_batch, label_batch, CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
